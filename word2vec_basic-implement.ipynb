{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "446776f3-9d99-4ddb-a9f9-49032adf57b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Shivani\n",
      "[nltk_data]     Agarwal\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\Shivani\n",
      "[nltk_data]     Agarwal\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data] Downloading package omw-1.4 to C:\\Users\\Shivani\n",
      "[nltk_data]     Agarwal\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "\n",
    "# Download NLTK data (if not already)\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "11b6ddae-5cbf-47ce-82c2-a7f31344e8ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77d27569-3b36-4f3f-9e9a-c34f8fc0031c",
   "metadata": {},
   "source": [
    "## Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7b77d283-2560-430e-b7c4-0416a5518508",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_manual(sentence):\n",
    "    sentence = sentence.lower()                           # lowercase\n",
    "    sentence = re.sub(r'[^\\w\\s]', '', sentence)          # (,.?/)remove punctuation\n",
    "    tokens = sentence.split()                             # tokenize\n",
    "    tokens = [w for w in tokens if w not in stop_words]  # (i,the,a,an)remove stopwords\n",
    "    tokens = [lemmatizer.lemmatize(w) for w in tokens]   # lemmatize\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21360cd4-6e86-40f1-a7ea-e52b7a4a1717",
   "metadata": {},
   "source": [
    "## Corpus For Preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f44bb4dc-21cf-49f2-b48a-7c62f0292efe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed corpus: [['love', 'natural', 'language', 'processing'], ['word2vec', 'creates', 'vector', 'representation', 'word'], ['gensim', 'make', 'working', 'word', 'embeddings', 'easy'], ['python', 'great', 'nlp', 'task']]\n"
     ]
    }
   ],
   "source": [
    "sentences = [\n",
    "    \"I love natural language processing\",\n",
    "    \"Word2Vec creates vector representations of words\",\n",
    "    \"Gensim makes working with word embeddings easy\",\n",
    "    \"Python is great for NLP tasks\"\n",
    "]\n",
    "\n",
    "tokenized_sentences = [preprocess_manual(s) for s in sentences]\n",
    "print(\"Preprocessed corpus:\", tokenized_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95502fb8-8121-40b0-91f6-e4e8421b4592",
   "metadata": {},
   "source": [
    "## Build Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1ea0fcf9-cd7e-4500-9573-b03060a7a7eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary: ['creates', 'easy', 'embeddings', 'gensim', 'great', 'language', 'love', 'make', 'natural', 'nlp', 'processing', 'python', 'representation', 'task', 'vector', 'word', 'word2vec', 'working']\n"
     ]
    }
   ],
   "source": [
    "# Flatten list of words\n",
    "words = [word for sentence in tokenized_sentences for word in sentence]\n",
    "vocab = sorted(set(words))\n",
    "\n",
    "word2idx = {word: i for i, word in enumerate(vocab)}\n",
    "idx2word = {i: word for word, i in word2idx.items()}\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "print(\"Vocabulary:\", vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c5940b8-96be-4152-ae6e-0db6e7a2fb4d",
   "metadata": {},
   "source": [
    "## Creating Training Data For CBOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0c25473b-a23c-4526-a32f-c1801e48e8dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example training pair: (['natural', 'language'], 'love')\n"
     ]
    }
   ],
   "source": [
    "def generate_cbow_data(corpus, window=2):\n",
    "    data = []\n",
    "    for sentence in corpus:\n",
    "        for i, target in enumerate(sentence):\n",
    "            context = []\n",
    "            for j in range(i - window, i + window + 1):\n",
    "                if j != i and j >= 0 and j < len(sentence):\n",
    "                    context.append(sentence[j])\n",
    "            if context:\n",
    "                data.append((context, target))\n",
    "    return data\n",
    "\n",
    "training_data = generate_cbow_data(tokenized_sentences)\n",
    "print(\"Example training pair:\", training_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c5c27bd-a2d8-4c2f-9e17-8dbe1734d76c",
   "metadata": {},
   "source": [
    "## One Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "058f130f-25ef-42ce-b422-1e467a6a6bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(word, word2idx):\n",
    "    vec = np.zeros(len(word2idx))\n",
    "    vec[word2idx[word]] = 1\n",
    "    return vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "640976a4-d49a-4bc1-91a0-744009857e33",
   "metadata": {},
   "source": [
    "## Initialize Model Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1d58d9aa-c190-4698-bf54-cdecbac0924e",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 10   # small for demo\n",
    "W1 = np.random.rand(vocab_size, embedding_dim)   # input -> hidden\n",
    "W2 = np.random.rand(embedding_dim, vocab_size)   # hidden -> output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d058d9c-865c-4861-b063-799ead2ac851",
   "metadata": {},
   "source": [
    "## Training CBOW Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e74a61e6-bfc1-4194-8d66-e5afc68a6e82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Loss: 40.4820\n",
      "Epoch 200, Loss: 24.2535\n",
      "Epoch 300, Loss: 14.6094\n",
      "Epoch 400, Loss: 9.6549\n",
      "Epoch 500, Loss: 7.1264\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.01\n",
    "epochs = 500\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    loss = 0\n",
    "    for context_words, target_word in training_data:\n",
    "        # 1. Average context vectors\n",
    "        x = np.mean([one_hot(w, word2idx) for w in context_words], axis=0)\n",
    "\n",
    "        # 2. Hidden layer\n",
    "        h = np.dot(W1.T, x)\n",
    "\n",
    "        # 3. Output layer (softmax)\n",
    "        u = np.dot(W2.T, h)\n",
    "        y_pred = np.exp(u) / np.sum(np.exp(u))\n",
    "\n",
    "        # 4. True vector\n",
    "        y_true = one_hot(target_word, word2idx)\n",
    "\n",
    "        # 5. Loss (cross-entropy)\n",
    "        loss += -np.sum(y_true * np.log(y_pred + 1e-9))\n",
    "        \n",
    "        # 6. Backpropagation\n",
    "        e = y_pred - y_true\n",
    "        dW2 = np.outer(h, e)\n",
    "        dW1 = np.outer(x, np.dot(W2, e))\n",
    "\n",
    "        # 7. Update weights\n",
    "        W1 -= learning_rate * dW1\n",
    "        W2 -= learning_rate * dW2\n",
    "\n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        print(f\"Epoch {epoch+1}, Loss: {loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f409aa40-503f-451d-ac0d-5ce79f9eede6",
   "metadata": {},
   "source": [
    "## Extract Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d049b5c5-a187-4831-9bc8-13ef93f44bfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector for 'python': [ 1.64131423  0.95906346  1.29693459  0.877179   -0.62526354  0.48917783\n",
      " -0.15221688  0.46896776  0.9260571   0.5868147 ]\n"
     ]
    }
   ],
   "source": [
    "def get_vector(word):\n",
    "    return W1[word2idx[word]]\n",
    "\n",
    "print(\"Vector for 'python':\", get_vector('python'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90a22715-e646-4772-8a0a-75547b1b8fd6",
   "metadata": {},
   "source": [
    "## Generate Skip-Gram Training Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "27db9eeb-eedb-4cb7-8cd3-c719814cf50a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example Skip-Gram pair: ('love', 'natural')\n"
     ]
    }
   ],
   "source": [
    "# Create training data for Skip-Gram\n",
    "def generate_skipgram_data(corpus, window=2):\n",
    "    data = []\n",
    "    for sentence in corpus:\n",
    "        for i, target in enumerate(sentence):\n",
    "            for j in range(i - window, i + window + 1):\n",
    "                if j != i and j >= 0 and j < len(sentence):\n",
    "                    context = sentence[j]\n",
    "                    data.append((target, context))\n",
    "    return data\n",
    "\n",
    "training_data_sg = generate_skipgram_data(tokenized_sentences)\n",
    "print(\"Example Skip-Gram pair:\", training_data_sg[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "074f7da9-b18b-46f8-afd5-ce1ccf1fedba",
   "metadata": {},
   "source": [
    "## Train Skip-Gram Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0a27ae85-52bc-473b-869e-c7eef8dd2e44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100 — Loss: 82.4831\n",
      "Epoch 200 — Loss: 64.5488\n",
      "Epoch 300 — Loss: 62.2612\n",
      "Epoch 400 — Loss: 61.6073\n",
      "Epoch 500 — Loss: 61.3477\n"
     ]
    }
   ],
   "source": [
    "# Initialize weights again for Skip-Gram\n",
    "embedding_dim = 10\n",
    "W1_sg = np.random.rand(vocab_size, embedding_dim)\n",
    "W2_sg = np.random.rand(embedding_dim, vocab_size)\n",
    "\n",
    "learning_rate = 0.01\n",
    "epochs = 500\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    loss = 0\n",
    "    for target_word, context_word in training_data_sg:\n",
    "        \n",
    "        # 1️⃣ Input vector (target)\n",
    "        x = one_hot(target_word, word2idx)\n",
    "\n",
    "        # 2️⃣ Hidden layer\n",
    "        h = np.dot(W1_sg.T, x)\n",
    "\n",
    "        # 3️⃣ Output layer (Softmax)\n",
    "        u = np.dot(W2_sg.T, h)\n",
    "        y_pred = np.exp(u) / np.sum(np.exp(u))\n",
    "\n",
    "        # 4️⃣ True output vector (context)\n",
    "        y_true = one_hot(context_word, word2idx)\n",
    "\n",
    "        # 5️⃣ Loss\n",
    "        loss += -np.sum(y_true * np.log(y_pred + 1e-9))\n",
    "\n",
    "        # 6️⃣ Backpropagation\n",
    "        e = y_pred - y_true\n",
    "        dW2 = np.outer(h, e)\n",
    "        dW1 = np.outer(x, np.dot(W2_sg, e))\n",
    "\n",
    "        # 7️⃣ Update weights\n",
    "        W1_sg -= learning_rate * dW1\n",
    "        W2_sg -= learning_rate * dW2\n",
    "\n",
    "    if (epoch+1) % 100 == 0:\n",
    "        print(f\"Epoch {epoch+1} — Loss: {loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b40506af-4048-44fd-bd08-58f2e1db7ae2",
   "metadata": {},
   "source": [
    "## Get Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7ae3efa8-365b-4d87-ae5b-9b7d7bd791fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Skip-Gram vector for 'python':\n",
      "[ 0.8433671   1.06691425  0.21895965 -0.46480289  0.16026096 -0.42141261\n",
      "  1.55643261  1.84405757 -0.42162315  0.3000474 ]\n"
     ]
    }
   ],
   "source": [
    "def get_vector_sg(word):\n",
    "    return W1_sg[word2idx[word]]\n",
    "\n",
    "print(\"\\nSkip-Gram vector for 'python':\")\n",
    "print(get_vector_sg('python'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbc23604-4614-48d4-a57b-789580f2aadf",
   "metadata": {},
   "source": [
    "## Comparison CBOW VS Skip-Gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9fd710d9-496c-4596-ae4f-272bdd20dfdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CBOW: [ 1.64131423  0.95906346  1.29693459  0.877179   -0.62526354  0.48917783\n",
      " -0.15221688  0.46896776  0.9260571   0.5868147 ]\n",
      "Skip-Gram: [ 0.8433671   1.06691425  0.21895965 -0.46480289  0.16026096 -0.42141261\n",
      "  1.55643261  1.84405757 -0.42162315  0.3000474 ]\n"
     ]
    }
   ],
   "source": [
    "print(\"CBOW:\", get_vector('python'))\n",
    "print(\"Skip-Gram:\", get_vector_sg('python'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "188bc6bb-c3b4-4e52-bc45-eeccb404110d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
